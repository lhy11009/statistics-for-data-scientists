{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Data and Sampling Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an unknown distribution (population) vs a random sampling \n",
    "\n",
    "Traditional statistics: theory based on assumptions about the population.\n",
    "\n",
    "Modrn statistics: using sampling directly, where such assumptions are not needed.\n",
    "\n",
    "Random sampling:\n",
    "* with replacement: observations are put back and can be selected multiple times\n",
    "* without: observations once selected are unavailable\n",
    "\n",
    "Quality of sampling is important\n",
    "* Literary Digest poll of 1936 that predicted a victory of Alf Landon over Franklin Roosevelt with 10 million people.\n",
    "* George Gallup, founder of the Gallup Poll, conducted biweekly polls of just 2,000 people and accurately predicted a Roosevelt victory\n",
    "\n",
    "The result was sample bias; that is, the sample was different in some meaningful and nonrandom way from the larger population\n",
    "it was meant to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load libraies\n",
    "list.of.packages <- c(\"boot\", \"ggplot2\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages)\n",
    "\n",
    "library(boot)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "PSDS_PATH <- file.path('~/Desktop', 'statistics-for-data-scientists')\n",
    "\n",
    "loans_income <- read.csv(file.path(PSDS_PATH, 'data', 'loans_income.csv'))[,1]\n",
    "sp500_px <- read.csv(file.path(PSDS_PATH, 'data', 'sp500_px.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- seq(from=-3, to=3, length=300)\n",
    "gauss <- dnorm(x)\n",
    "\n",
    "# png(filename=file.path(PSDS_PATH, 'figures', 'normal_density.png'),  width = 4, height=5, units='in', res=300)\n",
    "par(mar=c(3, 3, 0, 0)+.1)\n",
    "plot(x, gauss, type=\"l\", col='blue', xlab='', ylab='', axes=FALSE)\n",
    "polygon(x, gauss, col='blue')\n",
    "# dev.off()\n",
    "\n",
    "# png(filename=file.path(PSDS_PATH, 'figures', 'samp_hist.png'), width = 200, height = 250)\n",
    "norm_samp <- rnorm(100)\n",
    "par(mar=c(3, 3, 0, 0)+.1)\n",
    "hist(norm_samp, axes=FALSE, col='red', main='')\n",
    "# dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random error and bias error\n",
    "\n",
    "Random error: unbiased process of gun shooting produces error (2-2).\n",
    "\n",
    "Biased error: shots shift to the upper right corner.\n",
    "\n",
    "\n",
    "![](02.png)\n",
    "![](03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample mean versus population mean\n",
    "\n",
    "sample mean: $\\=x$, from observation\n",
    "\n",
    "population mean: $\\mu$, inferred from a population\n",
    "\n",
    "### Selection bias and data snooping\n",
    "\n",
    "Selection bias: Bias resulting from the way in which observations are selected.\n",
    "\n",
    "Data snooping: Extensive hunting through data in search of something interesting.\n",
    "\n",
    "    “If you torture the data long enough, sooner or later it will confess.”\n",
    "\n",
    "### The vast search effecct\n",
    "\n",
    "One person toss coins 10 times and receive all head-up -> his special talent\n",
    "\n",
    "people in a stadium toss conis -> 99% someone get 10 heads\n",
    "\n",
    "again this: holdout sets or target shuffling (a permutation test).\n",
    "\n",
    "### Regression to the mean\n",
    "\n",
    "Extreme observations tend to be followed by more central ones.\n",
    "\n",
    "e.g. tall parents tend to have not so tall kids.\n",
    "\n",
    "![](04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the sample vs distribution of mean values of sampling\n",
    "\n",
    "The mean is likely to be more regular and bell-shaped than the distribution of the data itself.\n",
    "\n",
    "a sample of 1,000 values, a sample of 1,000 means of 5 values, and a sample of 1,000 means of 20 values (below).\n",
    "\n",
    "### Central limit theorem\n",
    "\n",
    "It says that the means drawn from multiple samples will resemble the familiar bell-shaped normal curve (see “Normal Distribution” on page 69), even if the source population is\n",
    "not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great.\n",
    "\n",
    "The central limit theorem allows normal-approximation formulas like the t-distribution to be used in calculating sampling distributions for inference—that is, confidence intervals and hypothesis\n",
    "tests.\n",
    "\n",
    "In practice: bootstrap is available, which doesn't require the CLT in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Code snippet 2.1\n",
    "\n",
    "# take a simple random sample\n",
    "samp_data <- data.frame(income=sample(loans_income, 1000), \n",
    "                        type='data_dist')\n",
    "# take a sample of means of 5 values\n",
    "samp_mean_05 <- data.frame(\n",
    "  income = tapply(sample(loans_income, 1000*5), \n",
    "                  rep(1:1000, rep(5, 1000)), FUN=mean),\n",
    "  type = 'mean_of_5')\n",
    "# take a sample of means of 20 values\n",
    "samp_mean_20 <- data.frame(\n",
    "  income = tapply(sample(loans_income, 1000*20), \n",
    "                  rep(1:1000, rep(20, 1000)), FUN=mean),\n",
    "  type = 'mean_of_20')\n",
    "# bind the data.frames and convert type to a factor\n",
    "income <- rbind(samp_data, samp_mean_05, samp_mean_20)\n",
    "income$type = factor(income$type, \n",
    "                     levels=c('data_dist', 'mean_of_5', 'mean_of_20'),\n",
    "                     labels=c('Data', 'Mean of 5', 'Mean of 20'))\n",
    "# plot the histograms\n",
    "ggplot(income, aes(x=income)) +\n",
    "  geom_histogram(bins=40) +\n",
    "  facet_grid(type ~ .)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error\n",
    "\n",
    "Standard error = SE = $\\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "to reduce the standard error by a factor of 2, the sample size must be increased by a factor of 4\n",
    "\n",
    "(not s is already the sandard deviation of the sample)\n",
    "\n",
    "The validity of the standard error formula arises from the central limit theorem.\n",
    "\n",
    "By definition, to compute the standard error, need to sample new sets multiple times, calculate the deviation and then derive the standard deviation.\n",
    "\n",
    "In practice, bootstrap is used to estimate the standard error, and doesn't reply on the central limit theorem.\n",
    "\n",
    "### The Bootstrap\n",
    "\n",
    "draw additional samples with replacement from the sample itself.\n",
    "\n",
    "(make a synthetic \"population\" with replicates of the sample instead of drawing from the original population)\n",
    "\n",
    "![](05.png)\n",
    "\n",
    "In practice, this is done by drawing with replacement (thus the sample / \"population\" remains unchanged)\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1. Draw a sample value, record it, and then replace it.\n",
    "2. Repeat n times.\n",
    "3. Record the mean of the n resampled values.\n",
    "4. Repeat steps 1–3 R times.\n",
    "5. Use the R results to:\n",
    "    * a. Calculate their standard deviation (this estimates sample mean standard\n",
    "error).\n",
    "    * b. Produce a histogram or boxplot.\n",
    "    * c. Find a confidence interval.\n",
    "\n",
    "In R , this is combined in the \"boot\" function\n",
    "\n",
    "The function stat_fun computes the median for a given sample specified by the\n",
    "index idx.\n",
    "\n",
    "The original estimate of the median is $62,000. The bootstrap distribution indicates\n",
    "that the estimate has a bias of about –$70 and a standard error of $209.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stat_fun <- function(x, idx) median(x[idx])\n",
    "boot_obj <- boot(loans_income, R = 1000, statistic=stat_fun)\n",
    "boot_obj # print result as object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept of \"Bagging\"\n",
    "\n",
    "short for “bootstrap aggregating”; see “Bagging and the Random Forest” on page 259\n",
    "\n",
    "Multivariat data -> sample the rows as units (bootstrap) -> use the bootstrap sample to run multiple prediction trees -> averaging their predictions -> better performance\n",
    "\n",
    "\n",
    "![](06.png)\n",
    "\n",
    "#### understanding the bootstrap\n",
    "\n",
    "The bootstrap does not compensate for a small sample size; it does\n",
    "not create new data, nor does it fill in holes in an existing data set.\n",
    "It merely informs us about how lots of additional samples would\n",
    "behave when drawn from a population like our original sample.\n",
    "\n",
    "#### Bootstrapping vs resampling\n",
    "\n",
    "Synonymous in most case\n",
    "\n",
    "In addition, resampling could include permutation procedures where mutiple samples are not replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "90% condifidence: the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Draw a random sample of size n with replacement from the data (a resample).\n",
    "2. Record the statistic of interest for the resample.\n",
    "3. Repeat steps 1–2 many (R) times.\n",
    "4. For an x% confidence interval, trim [(100-x) / 2]% of the R resample results from\n",
    "either end of the distribution.\n",
    "5. The trim points are the endpoints of an x% bootstrap confidence interval.\n",
    "\n",
    "* Confidence intervals are the typical way to present estimates as an interval range.\n",
    "•* The more data you have, the less variable a sample estimate will be.\n",
    "\n",
    "![](07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution\n",
    "\n",
    "Most of the variables follow a normal distribution -> when empirical probability distributions, or bootstrap distributions,\n",
    "are not available -> use a normal distribution instead.\n",
    "\n",
    "It's also termed a Gaussian distribution, attributed to Carl Friddrich Gauss\n",
    "\n",
    "#### normalization / standardization\n",
    "\n",
    "subtract the mean and then divide by the standard deviation; this is also called normalization or standardization\n",
    "\n",
    "#### QQ-Plot\n",
    "\n",
    "determine how close a sample is to a specified distribution\n",
    "\n",
    "The QQ-Plot orders the z-scores from low to high and plots each value’s z-score on the y-axis; the x-axis is the corresponding quantile of a normal distribution for that value’s rank.\n",
    "\n",
    "To convert data to z-scores, you subtract the mean of the data and divide by the standard deviation; you can then compare the data to a normal distribution.\n",
    "\n",
    "(to derive the plot, you need the quantile of each point in the sampling and in a normal distribution.)\n",
    "\n",
    "QQ-plot for 100 values generate from a normal distribution -> closely follow the line (indicating normal distribution).\n",
    "\n",
    "(The \"abline\" function seems to plot a line on the canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Code for Figure 11\n",
    "# png(filename=file.path(PSDS_PATH, 'figures', 'psds_0211.png'),  width = 4, height=4, units='in', res=300)\n",
    "norm_samp <- rnorm(100)\n",
    "par(mar=c(3, 3, 0, 0)+.1)\n",
    "qqnorm(norm_samp, main='', xlab='', ylab='')\n",
    "abline(a=0, b=1, col='grey')\n",
    "# dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The long-tailed distribution\n",
    "\n",
    "Tail: The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.\n",
    "\n",
    "skewed: the data distribution is asymmetric.\n",
    "\n",
    "discrete: e.g. bimordial distribution\n",
    "\n",
    "The black swan theory by Nassim Taleab: anomalous events, such as a stock market crash, are much more likely to occur than would be predicted by the normal distribution.\n",
    "\n",
    "Example: QQ-Plot for the daily stock returns for Netflix (NFLX).\n",
    "\n",
    "The z scored value is higher than the normal distribution value in magnitude at the end, indicating a long tail distribution.\n",
    "\n",
    "The points are close to the line for the data within one standard deviation of the mean, this is refered as data being \"normal in the middle\" but having long tails.\n",
    "\n",
    "(this illustrate the point in practice, we need to first have a sense of the sample distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Code for Figure 12\n",
    "# png(filename=file.path(PSDS_PATH, 'figures', 'psds_0212.png'),  width = 4, height=4, units='in', res=300)\n",
    "par(mar=c(3, 3, 0, 0)+.1)\n",
    "nflx <- sp500_px[,'NFLX']\n",
    "nflx <- diff(log(nflx[nflx>0]))\n",
    "qqnorm(nflx, main='', xlab='', ylab='')\n",
    "abline(a=0, b=1, col='grey')\n",
    "# dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student’s t-Distribution\n",
    "\n",
    "* a normally shaped distribution, except that it is a bit thicker and longer on the tails.\n",
    "* used extensively in depicting distributions of sample statistics.\n",
    "* there is a family of t-distributions that differ depending on how large the sample is.\n",
    "\n",
    "Gosset's question: “What is the sampling distribution of the mean of a sample, drawn from a larger population?”\n",
    "* resampling experiment—drawing random samples of 4 from a data set of 3,000 measurements of criminals’ height and left-middle-finger length.\n",
    "* plot z-scores on the x-axis and frequency on the y-axis.\n",
    "* invent a function to fit the distribution (Student's t)\n",
    "\n",
    "![](08.png)\n",
    "\n",
    "Usage of the t-distribution: estimate confidence intervals to show sampling variation\n",
    "* compute the mean of sample\n",
    "* estimate a 90% confidence interval by:\n",
    "\n",
    "(it's a method when we cannot access the power of computer)\n",
    "\n",
    "![](09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution\n",
    "\n",
    "Binomial: 0 and 1 (yes or no)\n",
    "\n",
    "1 usually means the success output\n",
    "\n",
    "Binomial Distribution: Distribution of number of successes in n trials (possiblity of success: p).\n",
    "\n",
    "Example: R function \n",
    "* dbiom - the probability of observing exactly x = 2 successes in size = 5 trials\n",
    "* pbiom - the probability of observing two or fewer successes in five trials, where the probability of success for each trial is 0.1\n",
    "\n",
    "\n",
    "#### mean, variance\n",
    "\n",
    "* mean - n*p\n",
    "* variance - n*p(1-p)\n",
    "* approximantion - a normal distribution with mean and variance value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dbinom(x=2, size=5, p=0.1)\n",
    "pbinom(2, 5, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square distribution\n",
    "\n",
    "measure whether the sample matches an expected distribution (variation in group menas matches \"normal\" random variation)\n",
    "\n",
    "for discrete values\n",
    "\n",
    "### F-distribution\n",
    "\n",
    "for continous values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "distribution of events per unit of time or space\n",
    "\n",
    "e.g. How much capacity do we need to be 95% sure of fully processing the internet traffic that arrives on a server in any fivesecond period?\n",
    "\n",
    "$\\lambda$ - This is the mean number of events that occurs in a specified interval of time or space\n",
    "\n",
    "Example: generate 100 random numbers from a Poisson distribution with λ = 2\n",
    "\n",
    "For example, if incoming customer service calls average two per minute, this code will simulate 100 minutes, returning the number of calls in each of those 100 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rpois(100, lambda=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution\n",
    "\n",
    "similar as the possion distribution, model distribution of the time between events: time between visits to a website or\n",
    "between cars arriving at a toll plaza.\n",
    "\n",
    "Example: n = 100, rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rexp(n=100, rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weibull Distribution\n",
    "\n",
    "if event rate changes\n",
    "\n",
    "if typical interval is shorter than the period over which it changes -> good -> break the time period into small enough intervals and apply poisson distribution in each.\n",
    "\n",
    "if the period over which it changes is shorter than typical interval -> weibull distribution (e.g. mechnical failure, risk of failure increases)\n",
    "\n",
    "a shape parameter, β. If β > 1, the probability of an event increases over time; if β < 1, the probability decreases.\n",
    "\n",
    "characteristic life (scale parameter), $\\eta$ \n",
    "\n",
    "example: 100 random numbers (lifetimes) from a Weibull distribution with shape of 1.5 and characteristic life of 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rweibull(100, 1.5, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Failure Rate\n",
    "\n",
    "With rate data or no data at all: e.g. Aircraft engine failure\n",
    "\n",
    "idea: if no events have been seen after 20 hours, you can be pretty sure that the rate is not 1 per hour.\n",
    "\n",
    "-> estimate a rate about which it's unlikely\n",
    "\n",
    "-> a goodness-of-fit test with the estimated rate (Chi-square test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* Michael Harris’s article “Fooled by Randomness Through Selection Bias” provides\n",
    "an interesting review of selection bias considerations in stock market trading\n",
    "schemes, from the perspective of traders.\n",
    "\n",
    "* The Black Swan, 2nd ed., by Nassim Nicholas Taleb (Random House, 2010)\n",
    "\n",
    "* Handbook of Statistical Distributions with Applications, 2nd ed., by K. Krishnamoorthy\n",
    "(Chapman & Hall/CRC Press, 2016)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
